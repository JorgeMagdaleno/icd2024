{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e99d4ff-7ff6-4f6d-aa84-49a2d0bbc584",
   "metadata": {},
   "source": [
    "# Notebook Bag of words\n",
    "Jorge Antonio Magdaleno Rodriguez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ac246-c8bf-46a7-a799-90ad4d63777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec0aec-430d-4eaa-a44f-7139a92e7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy as spc\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37440f76-a7f1-4c47-9e8b-1710752c14c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  ESAS COSAS Y OTRAS PUEDEN PASAR POR MANTENER A...\n",
      "1      1  28: te amodio, odio a la perra de tu amiga ☺️☺...\n",
      "2      1  @LaDivinaDiva Callate maldita perra. O seguro ...\n",
      "3      1  @MarysabelPuerto Mejor callate cara de puta o ...\n",
      "4      1  @xarita327 @TRIKYHUMOR @yonier2012 @casTa1326 ...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('df_mini_HS.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e9c3e-1e69-47d4-aaea-dcc7611b1b9c",
   "metadata": {},
   "source": [
    "Limpiamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af29221-7326-4b54-8df8-9320c98a478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                                                    1\n",
      "text      te amodio odio a la perra de tu amiga pero tu...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "def remove_accents(oracion):\n",
    "    oracion = unicodedata.normalize('NFD', oracion)\n",
    "    oracion = ''.join(char for char in oracion if unicodedata.category(char) != 'Mn')\n",
    "    return oracion\n",
    "\n",
    "def clean_text(oracion):\n",
    "    oracion = re.sub(r\"@\\S+\", \"\", oracion)  # Eliminar menciones a usuarios\n",
    "    oracion = re.sub(r\"http[s]?\\://\\S+\", \"\", oracion)  # Eliminar enlaces\n",
    "    oracion = re.sub(r\"#\\S+\", \"\", oracion)  # Eliminar hashtags\n",
    "    oracion = re.sub(r\"[0-9]\", \"\", oracion)  # Eliminar números\n",
    "    oracion = re.sub(r\"(\\(.*\\))|(\\[.*\\])\", \"\", oracion)  # Eliminar paréntesis y corchetes\n",
    "    oracion = re.sub(r\"\\n\", \"\", oracion)  # Eliminar caracteres de nueva línea\n",
    "    oracion = re.sub(r\"(http[s]?\\://\\S+)|([\\[\\(].*[\\)\\]])|([#@]\\S+)|\\n\", \"\", oracion)  # Eliminar varios patrones\n",
    "    oracion = re.sub(r\"(\\.)|(,)\", \"\", oracion)  # Eliminar puntos y comas\n",
    "    oracion = re.sub(r\"[¡!]\", \"\", oracion)  # Eliminar signos de admiración \n",
    "    oracion = re.sub(r\"[¿?]\", \"\", oracion)  # Eliminar signos de interrogación\n",
    "    oracion = re.sub(r\"[:]\", \"\", oracion)  # Eliminar dos puntos\n",
    "    oracion = re.sub(r\"[\\\"]\", \"\", oracion)  # Eliminar dos comillas\n",
    "    oracion = re.sub(r\"[\\/]\", \"\", oracion)  # Eliminar diagonales\n",
    "    oracion = re.sub(r\"[*]\", \"\", oracion)  # Eliminar asteriscos\n",
    "    oracion = re.sub(r\"[\"\n",
    "                     u\"\\U0001F600-\\U0001F64F\"  # Emoticonos\n",
    "                     u\"\\U0001F300-\\U0001F5FF\"  # Símbolos y pictogramas\n",
    "                     u\"\\U0001F680-\\U0001F6FF\"  # Transporte y símbolos de mapas\n",
    "                     u\"\\U0001F1E0-\\U0001F1FF\"  # Banderas\n",
    "                     u\"\\U00002702-\\U000027B0\"  # Otros símbolos\n",
    "                     u\"\\U000024C2-\\U0001F251\"  # Más símbolos\n",
    "                     \"]+\", \"\", flags=re.UNICODE, string=oracion)\n",
    "    oracion = remove_accents(oracion)\n",
    "    return oracion\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64de6a-68b4-4bb7-a88e-0b7235ca2c17",
   "metadata": {},
   "source": [
    "Los tokenizamos y filtramos los stopwords. Los guardamos dentro del mismo df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85959bc-44fe-4638-9120-ec80a75ae4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [cosas, pueden, pasar, mantener, inmigracion, ...\n",
      "1          [amodio, odio, perra, amiga, vida, tijerazo]\n",
      "2     [callate, maldita, perra, seguro, pobre, maric...\n",
      "3     [mejor, callate, cara, puta, reputa, madre, no...\n",
      "4                                       [callate, puta]\n",
      "5                                       [callate, puta]\n",
      "6     [inmigrante, recibe, ayuda, rico, ladron, olvi...\n",
      "7     [moros, puede, esperar, bueno, dicen, propios,...\n",
      "8     [si, mujer, pegan, tiro, cabeza, dura, tres, d...\n",
      "9     [analicemos, si, pones, shorts, asi, calle, es...\n",
      "10    [see, tal, vez, recordo, peron, protegio, eich...\n",
      "11    [pietrapierce, story, purs, sangs, arabes, sta...\n",
      "12    [dice, frivolizar, acoso, escolar, favor, quer...\n",
      "13    [retira, permiso, refugiados, vacaciones, pais...\n",
      "14    [hoy, quiero, denunciaaaaaaar, gente, puto, gu...\n",
      "15    [redomicilie, sociedad, offshore, emiratos, ar...\n",
      "16    [basta, poned, pie, pared, tanta, provocacion,...\n",
      "17    [semana, juventud, torneo, futbol, futbol, cat...\n",
      "18                  [callate, metete, party, puta, vez]\n",
      "19    [cuantos, inmigrantes, creemos, cuantos, reali...\n",
      "Name: filtered_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#nltk.data.path.append('/Users/jorgemagdaleno/nltk_data')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "def remove_stopwords(oracion):\n",
    "    tokens = word_tokenize(oracion)\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    combined_stopwords = set(spanish_stopwords + english_stopwords)\n",
    "    palabras_filtradas = [palabra for palabra in tokens if palabra.lower() not in combined_stopwords]\n",
    "    return palabras_filtradas  # Return as a list\n",
    "\n",
    "df['filtered_text'] = df['text'].apply(remove_stopwords)\n",
    "\n",
    "print(df['filtered_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e437a0f-ffb0-420a-a608-55cfac52b859",
   "metadata": {},
   "source": [
    "Hacemos la lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02b17a1-e7cb-47b0-b02e-752a4c7df868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     cosa poder pasar mantener inmigracion ilegal e...\n",
      "1                 amodio odio perra amigo vida tijerazo\n",
      "2     callate maldito perra seguro pobre marico detr...\n",
      "3     mejor callate cara puta reputa madre nota hace...\n",
      "4                                          callate puto\n",
      "5                                          callate puto\n",
      "6     inmigrante recibir ayuda rico ladron olvidar n...\n",
      "7     moro poder esperar bueno decir propio arab tam...\n",
      "8     si mujer peguir tiro cabeza durar tres dia mor...\n",
      "9     analicer si pón shorts asi calle esperas decir...\n",
      "10    see tal vez recordo peron protegio eichmann ci...\n",
      "11    pietrapiercir story purs sang arab stars des f...\n",
      "12    decir frivolizar acoso escolar favor quereis d...\n",
      "13          retirar permiso refugiado vacación país via\n",
      "14    hoy quiero denunciaaaaaaar gente puto guarra o...\n",
      "15     redomicilie sociedad offshore emirato arab unido\n",
      "16    bastar poned pie pared tanto provocacion corta...\n",
      "17    semana juventud torneo futbol futbol categoria...\n",
      "18                        callate metete party puta vez\n",
      "19    cuanto inmigrante creer cuanto realidad ciudad...\n",
      "Name: lemmatized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "nlp = spc.load(\"es_core_news_sm\")\n",
    "\n",
    "def lemmatize_text(filtered_text):\n",
    "    lema_p = nlp(\" \".join(filtered_text))\n",
    "    oracion_lematizada_p = \" \".join([token.lemma_ for token in lema_p])\n",
    "    return oracion_lematizada_p\n",
    "\n",
    "df['lemmatized_text'] = df['filtered_text'].apply(lemmatize_text)\n",
    "print(df['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c285abd-1144-4669-a1d1-0f34e18ea531",
   "metadata": {},
   "source": [
    "Vectorizamos y guardamos dentro de df_vectorized, mostramos los valores del bag of words de la segunda frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6ae20d-ab1d-4b87-876f-200e971a535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración de entrada:  te amodio odio a la perra de tu amiga pero tu eres mi vida tijerazo\n",
      "Oración lematizada: amodio odio perra amigo vida tijerazo\n",
      "Vectores Bag of Words: [1 1 1 1 1 1]\n",
      "Vocabulario: Index(['amigo', 'amodio', 'odio', 'perra', 'tijerazo', 'vida'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def vectorize_column(df, column_name):\n",
    "    vectorizador = CountVectorizer()\n",
    "    vectores = vectorizador.fit_transform(df[column_name])\n",
    "    vectorized_df = pd.DataFrame(vectores.toarray(), columns=vectorizador.get_feature_names_out())\n",
    "    #df = pd.concat([df, vectorized_df], axis=1)\n",
    "    return vectorized_df\n",
    "    \n",
    "df_vectorized = vectorize_column(df, 'lemmatized_text')\n",
    "\n",
    "print(\"Oración de entrada:\", df['text'].loc[1])\n",
    "print(\"Oración lematizada:\", df['lemmatized_text'].loc[1])\n",
    "print(\"Vectores Bag of Words:\", df_vectorized.loc[1][df_vectorized.loc[1] != 0].values)\n",
    "print(\"Vocabulario:\", df_vectorized.loc[1][df_vectorized.loc[1] != 0].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d53351-3c8d-4914-9d4c-8fb899a07aef",
   "metadata": {},
   "source": [
    "Finalmente agregamos la clase basado en el label y tenemos nuestro bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cfe32e9-79b8-4430-b5da-fe85f41896c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>acoso</th>\n",
       "      <th>agar</th>\n",
       "      <th>agresion</th>\n",
       "      <th>amigo</th>\n",
       "      <th>amodio</th>\n",
       "      <th>analicer</th>\n",
       "      <th>aprieto</th>\n",
       "      <th>arab</th>\n",
       "      <th>arar</th>\n",
       "      <th>...</th>\n",
       "      <th>user</th>\n",
       "      <th>vacación</th>\n",
       "      <th>verdad</th>\n",
       "      <th>vez</th>\n",
       "      <th>via</th>\n",
       "      <th>vida</th>\n",
       "      <th>viola</th>\n",
       "      <th>voolka</th>\n",
       "      <th>yogurín</th>\n",
       "      <th>él</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Misogino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Misogino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Misogino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Misogino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Misogino</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      clase  acoso  agar  agresion  amigo  amodio  analicer  aprieto  arab  \\\n",
       "0  Misogino      0     0         1      0       0         0        1     0   \n",
       "1  Misogino      0     0         0      1       1         0        0     0   \n",
       "2  Misogino      0     0         0      0       0         0        0     0   \n",
       "3  Misogino      0     0         0      0       0         0        0     0   \n",
       "4  Misogino      0     0         0      0       0         0        0     0   \n",
       "\n",
       "   arar  ...  user  vacación  verdad  vez  via  vida  viola  voolka  yogurín  \\\n",
       "0     0  ...     0         0       0    0    1     0      0       0        0   \n",
       "1     0  ...     0         0       0    0    0     1      0       0        0   \n",
       "2     0  ...     1         0       0    0    0     0      0       0        0   \n",
       "3     0  ...     0         0       0    0    0     0      0       0        0   \n",
       "4     0  ...     0         0       0    0    0     0      0       0        0   \n",
       "\n",
       "   él  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   0  \n",
       "4   0  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_clase(label):\n",
    "    return 'Misogino' if label == 1 else 'otro'\n",
    "\n",
    "df_vectorized['clase'] = df['label'].apply(assign_clase)\n",
    "df_vectorized = df_vectorized[['clase'] + [col for col in df_vectorized.columns if col != 'clase']]\n",
    "\n",
    "df_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45530746-361e-4ab6-97d1-0b3614270695",
   "metadata": {},
   "source": [
    "Como conclusión, utilizar palabras y frases reales es complicado debido a que existen muchas faltas de ortografía, el uso de emojis, palabras que solo se usan en grupos pequeños y varios otros problemas. Es necesario encontrar formas que nos permitan normalizar nuestros datos a un formato que la computadora pueda realmente entender.\n",
    "\n",
    "Además, a lo largo de este proceso, hemos visto cómo aplicar técnicas de procesamiento de lenguaje natural como la tokenización, la eliminación de stopwords, la lematización y la vectorización, lo que nos ayuda a limpiar y transformar el texto en una representación más estructurada. Estos pasos son esenciales para mejorar la calidad de los datos textuales y, finalmente, hacerlos útiles para tareas de análisis más avanzadas o modelos de aprendizaje automático."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
